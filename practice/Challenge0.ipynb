{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice session**\n",
    "\n",
    "Build your own RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:13:35.361634Z",
     "start_time": "2019-01-16T10:13:34.989237Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "#import gym.envs.box2d.lunar_lander as ll\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three environments in this practical session.\n",
    "\n",
    "- [Blackjack](https://gym.openai.com/envs/Blackjack-v0/). The classic casino game.\n",
    "- [Inverted Pendulum](https://gym.openai.com/envs/Pendulum-v0/). The inverted pendulum swingup problem is a classic problem in the control literature. In this version of the problem, the pendulum starts in a random position, and the goal is to swing it up so it stays upright.\n",
    "- [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/). Land a lunar module on the moon (just like the Chinese recently did).\n",
    "\n",
    "<div class=\"alert alert-warning\"><b> Exercice:</b><br>\n",
    "    Use the previous classes and the \"getting started\" below to implement an agent that learns an optimal control policy for one or several environments. Start with blackjack for a toy problem then move on to one of the two other ones.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:24:06.133763Z",
     "start_time": "2019-01-16T10:24:03.878071Z"
    }
   },
   "outputs": [],
   "source": [
    "blackjack = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:34:36.248750Z",
     "start_time": "2019-01-16T10:34:36.243696Z"
    }
   },
   "outputs": [],
   "source": [
    "help(blackjack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:37:24.968683Z",
     "start_time": "2019-01-16T10:37:24.964547Z"
    }
   },
   "outputs": [],
   "source": [
    "# state = [player's current sum, dealer's one showing card, player has a usable ace]\n",
    "x = blackjack.reset()\n",
    "print(x)\n",
    "# actions = 0: stick (stop their turn), 1: hit (ask for an additional card)\n",
    "print(\"action space: \", blackjack.action_space)\n",
    "# rewards: -1 for loosing, +1 for winning\n",
    "y,r,d,_ = blackjack.step(0)\n",
    "print(y)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:13:37.630672Z",
     "start_time": "2019-01-16T10:13:37.622834Z"
    }
   },
   "outputs": [],
   "source": [
    "pendulum = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:13:38.220320Z",
     "start_time": "2019-01-16T10:13:38.214764Z"
    }
   },
   "outputs": [],
   "source": [
    "# State space, action space\n",
    "# x = (cos(theta), sin(theta), thetaDot)\n",
    "x = pendulum.reset()  # random initialization\n",
    "print(x)\n",
    "# action = applied torque\n",
    "a = [0.]\n",
    "print(\"action space type:\", pendulum.action_space)\n",
    "print(\"lower bound on torque:\", pendulum.action_space.low)\n",
    "print(\"upper bound on torque:\", pendulum.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:13:42.212063Z",
     "start_time": "2019-01-16T10:13:40.288958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transitions, rendering\n",
    "pendulum.render()\n",
    "for i in range(100):\n",
    "    y,r,d,_=pendulum.step(a)\n",
    "    pendulum.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this session, you will work with discrete action spaces so, even though the simulator takes continous actionsas we only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:13:47.962086Z",
     "start_time": "2019-01-16T10:13:46.327904Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simplified, discrete actions space\n",
    "actions = [[-2.], [0.], [2.]]\n",
    "for i in range(100):\n",
    "    y,r,d,_=pendulum.step(actions[2])\n",
    "    pendulum.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:13:51.027720Z",
     "start_time": "2019-01-16T10:13:51.005084Z"
    }
   },
   "outputs": [],
   "source": [
    "lunarLander = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:15:51.009649Z",
     "start_time": "2019-01-16T10:15:51.005126Z"
    }
   },
   "outputs": [],
   "source": [
    "# State space, action space\n",
    "# x = [position_x, \n",
    "#      position_y, \n",
    "#      velocity_x, \n",
    "#      velocity_y, \n",
    "#      angle, \n",
    "#      angular_velocity, \n",
    "#      left_leg_touches_ground, \n",
    "#      right_leg_touches_ground]\n",
    "# action 0: do nothing, \n",
    "# action 1: fire left orientation engine, \n",
    "# action 2: fire main engine,\n",
    "# action 3: fire right orientation engine\n",
    "x = lunarLander.reset()\n",
    "print(x)\n",
    "print(lunarLander.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:15:53.432449Z",
     "start_time": "2019-01-16T10:15:51.780722Z"
    }
   },
   "outputs": [],
   "source": [
    "lunarLander.render()\n",
    "for i in range(100):\n",
    "    y,r,d,_=lunarLander.step(np.random.randint(4))\n",
    "    lunarLander.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:16:54.943593Z",
     "start_time": "2019-01-16T10:16:54.932742Z"
    }
   },
   "outputs": [],
   "source": [
    "pendulum.close()\n",
    "lunarLander.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your RL agent\n",
    "\n",
    "Use the above examples, the previous classes and your imagination to experiment with your own RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
